{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b724ff",
      "metadata": {
        "collapsed": true,
        "id": "00b724ff",
        "outputId": "a62dfe70-1313-4bc2-adbc-954e022de5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
            "Requirement already satisfied: lxml in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "#Install\n",
        "\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1624d29f",
      "metadata": {
        "id": "1624d29f"
      },
      "outputs": [],
      "source": [
        "#Import\n",
        "\n",
        "import requests\n",
        "import lxml\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy as np\n",
        "from random import sample\n",
        "from math import log\n",
        "from statistics import mean"
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8316395f",
      "metadata": {
        "id": "8316395f"
      },
      "outputs": [],
      "source": [
        "#Crawling data\n",
        "\n",
        "text = []\n",
        "for i in range(100):\n",
        "    url = 'https://www.isna.ir/page/archive.xhtml?mn=12&wide=0&dy=23&ms=0&pi=%i&yr=1400'%(i)\n",
        "    f = requests.get(url)\n",
        "    soup = BeautifulSoup(f.content,'lxml')\n",
        "    context = soup.find('main',{'class':'main-content'}).find_all('img')\n",
        "    for img in context:\n",
        "        text.append(img['alt'])\n",
        "with open('C:/PRIVATE/Uni_master/nlp/isna_news.txt', 'w' , encoding=\"utf-8\") as f:\n",
        "    for item in text:\n",
        "        f.write(\"%s\\n\" %(item))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a4baa2e",
      "metadata": {
        "id": "2a4baa2e"
      },
      "outputs": [],
      "source": [
        "#Unicodes for regex\n",
        "\n",
        "persian_num = r'\\u06F0-\\u06F9'\n",
        "persian_alpha = '\\u0621-\\u0628\\u062A-\\u063A\\u0641-\\u0642\\u0644-\\u0648\\u064E-\\u0651\\u0655\\u067E\\u0686\\u0698\\u06A9\\u06AF\\u06BE\\u06CC'\n",
        "arabic_num = '\\u0660-\\u0669'\n",
        "arabic_char = '\\u0629\\u0643\\u0649-\\u064B\\u064D\\u06D5'\n",
        "punc = '\\u2024\\u061F\\u003F'\n",
        "space = '[\\u0020\\u2000-\\u200F\\u2028-\\u202F]'\n",
        "numbers = '[0-9\\u0660-\\u0669\\u06F0-\\u06F9]'\n",
        "characters = '[0-9\\u004E\\u0020\\u06F0-\\u06F9\\u0621-\\u0628\\u062A-\\u063A\\u0641-\\u0642\\u0644-\\u0648\\u064E-\\u0651\\u0655\\u067E\\u0686\\u0698\\u06A9\\u06AF\\u06BE\\u06CC\\u0660-\\u0669\\u0629\\u0643\\u0649-\\u064B\\u064D\\u06D5\\u2024\\u061F\\u003F]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97188ab8",
      "metadata": {
        "id": "97188ab8"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        self.data = []\n",
        "        self.cleaned_data = []\n",
        "        self.tokenized_data = []\n",
        "        self.frequencies_dict = {}\n",
        "        self.n_tokens = 0\n",
        "        self.n_unique_tokens = 0\n",
        "        self.prepared_data = []\n",
        "        \n",
        "        self.read_data(path)\n",
        "        self.clean_text()\n",
        "        self.tokenizer()\n",
        "        self.get_most_freqs()\n",
        "        self.handle_unknown()\n",
        "        \n",
        "    def read_data(self, p):\n",
        "        \n",
        "        with open(p, 'r' , encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                self.data.append(line)\n",
        "    \n",
        "    def clean_text(self):\n",
        "        \n",
        "        for news in self.data:\n",
        "            news_space = re.sub(space , ' ' , news)\n",
        "            pattern = re.compile(characters)\n",
        "            result = pattern.findall(news_space)\n",
        "            result = ''.join(result)\n",
        "            news_num = re.sub(numbers, 'N' , result)\n",
        "            news_num = '\\s ' + news_num + ' \\e'\n",
        "            '''for i in range(len(news_num)):\n",
        "                if(news_num[i] == 'N'):\n",
        "                    if(i != 0) and (news_num[i-1] != 'N'):\n",
        "                        news_num = news_num[:i] + ' ' + news_num[i:]\n",
        "                    if(i != -1) and (news_num[i+1] != 'N'):\n",
        "                        news_num = news_num[:i] + ' ' + news_num[i:]'''\n",
        "            self.cleaned_data.append(news_num)\n",
        "    \n",
        "    def tokenizer(self):\n",
        "        \n",
        "        for news in self.cleaned_data:\n",
        "            self.tokenized_data.append(news.split(' '))\n",
        "        for i in range(len(self.tokenized_data)):\n",
        "            for j in range(len(self.tokenized_data[i])):\n",
        "                if('N' in self.tokenized_data[i][j]):\n",
        "                    self.tokenized_data[i][j] = 'N'\n",
        "            if('' in self.tokenized_data[i]):\n",
        "                self.tokenized_data[i].remove('')\n",
        "    \n",
        "    def get_most_freqs(self):\n",
        "        \n",
        "        n = 0\n",
        "        for token in self.tokenized_data:\n",
        "            n += len(token)\n",
        "        self.n_tokens = n\n",
        "        for news in self.tokenized_data:\n",
        "            for token in news:\n",
        "                if(token in self.frequencies_dict.keys()):\n",
        "                    self.frequencies_dict[token] += 1\n",
        "                else:\n",
        "                    self.frequencies_dict[token] = 1\n",
        "        self.frequencies_dict = dict(sorted(self.frequencies_dict.items(), key=lambda item: item[1] , reverse=True)[:10000])\n",
        "        frequent = list(self.frequencies_dict.keys())[:200]\n",
        "        with open('Frequent.txt', 'w' , encoding=\"utf-8\") as f:\n",
        "            for item in frequent:\n",
        "                f.write(\"%s\\n\" %(item))\n",
        "        self.n_unique_tokens = len(self.frequencies_dict)\n",
        "        print('number of Tokens: %i' %(self.n_tokens))\n",
        "        print('number of unique Tokens: %i' %(self.n_unique_tokens))\n",
        "        \n",
        "    def handle_unknown(self):\n",
        "        \n",
        "        keys = self.frequencies_dict\n",
        "        for news in self.tokenized_data:\n",
        "            news_new = []\n",
        "            for token in news:\n",
        "                if(token not in keys):\n",
        "                    news_new.append('unk')\n",
        "                else:\n",
        "                    news_new.append(token)\n",
        "            self.prepared_data.append(news_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3933e47e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3933e47e",
        "outputId": "7e75eb11-4094-42cf-eb6d-54935026e0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of Tokens: 35090\n",
            "number of unique Tokens: 5684\n"
          ]
        }
      ],
      "source": [
        "#Process on all of data\n",
        "\n",
        "path = './isna_news.txt'\n",
        "all_data = DataProcessor(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "343e2ede",
      "metadata": {
        "id": "343e2ede"
      },
      "outputs": [],
      "source": [
        "#Splitting train and test\n",
        "\n",
        "Data = []\n",
        "with open(path, 'r' , encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        Data.append(line)\n",
        "Train = sample(Data , 2400)\n",
        "Test = []\n",
        "for x in Data:\n",
        "    if x not in Train:\n",
        "        Test.append(x)\n",
        "with open('C:/PRIVATE/Uni_master/nlp/news_train.txt', 'w' , encoding=\"utf-8\") as f:\n",
        "    for item in Train:\n",
        "        f.write(\"%s\" %(item))\n",
        "with open('C:/PRIVATE/Uni_master/nlp/news_test.txt', 'w' , encoding=\"utf-8\") as f:\n",
        "    for item in Test:\n",
        "        f.write(\"%s\" %(item))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070817c4",
      "metadata": {
        "id": "070817c4",
        "outputId": "616e6fc6-0371-4df2-d155-a87ad5495f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of Tokens: 28247\n",
            "number of unique Tokens: 5078\n"
          ]
        }
      ],
      "source": [
        "#Process on train data\n",
        "\n",
        "path_train = 'C:/PRIVATE/Uni_master/nlp/news_train.txt'\n",
        "train_pre = DataProcessor(path_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a16954",
      "metadata": {
        "id": "22a16954",
        "outputId": "7a1b3ae8-9810-4879-ffb4-377b8a868ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of Tokens: 6539\n",
            "number of unique Tokens: 2234\n"
          ]
        }
      ],
      "source": [
        "#Process on test data\n",
        "\n",
        "path_test = 'C:/PRIVATE/Uni_master/nlp/news_test.txt'\n",
        "test_pre = DataProcessor(path_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962ccb11",
      "metadata": {
        "id": "962ccb11"
      },
      "outputs": [],
      "source": [
        "class NgramLanguageModel:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.unigram_dict = train_pre.frequencies_dict\n",
        "        self.unigram_dict['unk'] = 0\n",
        "        self.bigram_dict = {}\n",
        "        self.trigram_dict = {}\n",
        "        self.most_probs = {}\n",
        "        self.prepared_test_data = test_pre.prepared_data\n",
        "        \n",
        "        self.make_ngram_dicts()\n",
        "    \n",
        "    def make_ngram_dicts(self):\n",
        "        \n",
        "        bigram = []\n",
        "        trigram = []\n",
        "        for news in train_pre.prepared_data:\n",
        "            news_new_bi = []\n",
        "            news_new_tri = []\n",
        "            for i in range(len(news)):\n",
        "                if(news[i] == 'unk'):\n",
        "                    self.unigram_dict['unk'] += 1\n",
        "                if(i != len(news)-1):\n",
        "                    news_new_bi.append(news[i] + ' ' + news[i+1])\n",
        "                if(i != len(news)-1) and (i != len(news)-2):\n",
        "                    news_new_tri.append(news[i] + ' ' + news[i+1] + ' ' + news[i+2])\n",
        "            bigram.append(news_new_bi)\n",
        "            trigram.append(news_new_tri)\n",
        "        \n",
        "        for news in bigram:\n",
        "            for token in news:\n",
        "                if(token in self.bigram_dict.keys()):\n",
        "                    self.bigram_dict[token] += 1\n",
        "                else:\n",
        "                    self.bigram_dict[token] = 1\n",
        "        self.bigram_dict = dict(sorted(self.bigram_dict.items(), key=lambda item: item[1] , reverse=True))\n",
        "        \n",
        "        for news in trigram:\n",
        "            for token in news:\n",
        "                if(token in self.trigram_dict.keys()):\n",
        "                    self.trigram_dict[token] += 1\n",
        "                else:\n",
        "                    self.trigram_dict[token] = 1\n",
        "        self.trigram_dict = dict(sorted(self.trigram_dict.items(), key=lambda item: item[1] , reverse=True))\n",
        "    \n",
        "    def removekey(self, d, key):\n",
        "        self.d = d\n",
        "        self.key = key\n",
        "        self.r = dict(self.d)\n",
        "        del self.r[self.key]\n",
        "        return self.r\n",
        "    \n",
        "    def calculate_smoothed_probs(self, n, Input):\n",
        "        \n",
        "        uni = self.removekey(self.unigram_dict, '\\s')\n",
        "        self.n = n\n",
        "        self.Input = Input\n",
        "        news_space = re.sub(space , ' ' , Input)\n",
        "        pattern = re.compile(characters)\n",
        "        result = pattern.findall(news_space)\n",
        "        result = ''.join(result)\n",
        "        news_num = re.sub(numbers, 'N' , result)\n",
        "        news_num =  news_num \n",
        "        news = news_num.split(' ')\n",
        "        for i in range(len(news)):\n",
        "            if('N' in news[i]):\n",
        "                news[i] = 'N'\n",
        "        \n",
        "        prob_bi = {}\n",
        "        if(self.n == 2):\n",
        "            if(news[-1] in uni.keys()):\n",
        "                for tok in uni.keys():\n",
        "                    token_bi = news[-1] + ' ' + tok\n",
        "                    if token_bi in self.bigram_dict.keys():\n",
        "                        prob_bi[tok] = (self.bigram_dict[token_bi] + 1)/(uni[news[-1]] + len(uni))\n",
        "                    else:\n",
        "                        prob_bi[tok] = (1)/(uni[news[-1]] + len(uni))\n",
        "            else:\n",
        "                for tok in uni.keys():\n",
        "                    token_bi = news[-1] + ' ' + tok\n",
        "                    if token_bi in self.bigram_dict.keys():\n",
        "                        prob_bi[tok] = (self.bigram_dict[token_bi] + 1)/(len(uni))\n",
        "                    else:\n",
        "                        prob_bi[tok] = (1)/(len(uni))\n",
        "            \n",
        "            self.most_probs = dict(sorted(prob_bi.items(), key=lambda item: item[1] , reverse=True)[:5])\n",
        "        \n",
        "        prob_tri = {}\n",
        "        if(self.n == 3):\n",
        "            prior = news[-2] + ' ' + news[-1]\n",
        "            if(prior in self.bigram_dict.keys()):\n",
        "                for tok in uni.keys():\n",
        "                    token = prior + ' ' + tok\n",
        "                    if token in self.trigram_dict.keys():\n",
        "                        prob_tri[tok] = (self.trigram_dict[token] + 1)/(self.bigram_dict[prior] + len(uni))\n",
        "                    else:\n",
        "                        prob_tri[tok] = (1)/(self.bigram_dict[prior] + len(uni))\n",
        "            else:\n",
        "                for tok in self.unigram_dict.keys():\n",
        "                    token = prior + ' ' + tok\n",
        "                    if token in self.trigram_dict.keys():\n",
        "                        prob_tri[tok] = (self.trigram_dict[token] + 1)/(len(uni))\n",
        "                    else:\n",
        "                        prob_tri[tok] = (1)/(len(uni))\n",
        "            \n",
        "            self.most_probs = dict(sorted(prob_tri.items(), key=lambda item: item[1] , reverse=True)[:5])\n",
        "        \n",
        "        return self.most_probs\n",
        "        \n",
        "    def generate_text(self, n, Input):\n",
        "        \n",
        "        self.n = n\n",
        "        self.Input = Input\n",
        "        self.probability = []\n",
        "        aa = []\n",
        "        i = 0\n",
        "        produced = list((self.calculate_smoothed_probs(self.n, self.Input)))[0]\n",
        "        self.Input += ' ' + produced\n",
        "        while(produced != '\\e' and i<len(self.unigram_dict)):\n",
        "            a = self.calculate_smoothed_probs(self.n, self.Input)\n",
        "            produced = list(a)[0]\n",
        "            aa.append(a[produced])\n",
        "            self.Input += ' ' + produced\n",
        "            i += 1\n",
        "            continue\n",
        "        return self.Input , aa\n",
        "        \n",
        "    def evaluate_model(self):\n",
        "        p_bi = []\n",
        "        p_tri = []\n",
        "        result_bi = []\n",
        "        result_tri = []\n",
        "        \n",
        "        test = self.prepared_test_data[:300]\n",
        "        for text in test:\n",
        "            if(len(text)>5):\n",
        "                p_bi = self.generate_text(2 , ' '.join(text[:5]))[1]\n",
        "                \n",
        "                p_tri = self.generate_text(3 , ' '.join(text[:5]))[1]\n",
        "                \n",
        "            \n",
        "            else:\n",
        "                p_bi = self.generate_text(2 , ' '.join(text))[1]\n",
        "                \n",
        "                p_tri = self.generate_text(3 , ' '.join(text))[1]\n",
        "                \n",
        "                \n",
        "            result_bi.append(sum([log(y,10) for y in p_bi])/len(text))\n",
        "            result_tri.append(sum([log(y,10) for y in p_tri])/len(text))\n",
        "        \n",
        "        if(mean(result_bi)>mean(result_tri)):\n",
        "            print('bigram is better than trigram')\n",
        "        else:\n",
        "            print('trigram is better than bigram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75d64b6",
      "metadata": {
        "id": "a75d64b6"
      },
      "outputs": [],
      "source": [
        "ngram = NgramLanguageModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bb0bde3",
      "metadata": {
        "id": "8bb0bde3",
        "outputId": "62aed504-e0ec-4e3c-8e1b-ae7cad8f3c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'با': 0.0005898545025560362,\n",
              " 'شیعیان': 0.0005898545025560362,\n",
              " '\\\\e': 0.00039323633503735744,\n",
              " 'طی': 0.00039323633503735744,\n",
              " 'مبارزه': 0.00039323633503735744}"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram.calculate_smoothed_probs(2 ,  'هیچ اپراتوری بدون هماهنگی')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb232a7",
      "metadata": {
        "id": "feb232a7",
        "outputId": "31b10dbb-3baf-4cd2-e0f2-276f8b191647"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'با': 0.0005905511811023622,\n",
              " '\\\\e': 0.0001968503937007874,\n",
              " 'در': 0.0001968503937007874,\n",
              " 'و': 0.0001968503937007874,\n",
              " 'N': 0.0001968503937007874}"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram.calculate_smoothed_probs(3 ,  'هیچ اپراتوری بدون هماهنگی')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e5c175",
      "metadata": {
        "id": "e1e5c175",
        "outputId": "3934f8bf-af21-4b75-8761-5538d5f5c018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('هیچ اپراتوری بدون هماهنگی با حضور در خراسان شمالی \\\\e',\n",
              " [0.0018772292096865028,\n",
              "  0.0009784735812133072,\n",
              "  0.004237978810105949,\n",
              "  0.006223259432127577,\n",
              "  0.0023464998044583495])"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram.generate_text(2 , 'هیچ اپراتوری بدون هماهنگی')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "097c4d1c",
      "metadata": {
        "id": "097c4d1c",
        "outputId": "c5db0ed6-2bb7-4811-c406-14ae477e8d16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('هیچ اپراتوری بدون هماهنگی با رگولاتوری حق افزایش تعرفه را ندارد \\\\e',\n",
              " [0.0005905511811023622,\n",
              "  0.0005905511811023622,\n",
              "  0.0005905511811023622,\n",
              "  0.0005905511811023622,\n",
              "  0.0005905511811023622,\n",
              "  0.0005905511811023622,\n",
              "  0.0005905511811023622])"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram.generate_text(3 , 'هیچ اپراتوری بدون هماهنگی')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ac1c66",
      "metadata": {
        "id": "82ac1c66",
        "outputId": "e8311135-27b7-4728-c7b1-9f9aba62235b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigram is better than trigram\n"
          ]
        }
      ],
      "source": [
        "ngram.evaluate_model()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "nlp_prac1.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
